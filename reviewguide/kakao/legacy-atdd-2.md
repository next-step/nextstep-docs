## 리뷰 철학
- 정답을 찾는 리뷰가 아니라 **의사결정을 검증하는 리뷰**입니다.
- “왜 그렇게 선택했는가?”에 집중합니다.
- 자동화 자체보다 **사고 과정과 전략**을 봅니다.

## 학습 목표
0. 1일차 경험을 바탕으로 프로젝트 재분석
1. 리스크 기반 인수조건 정의
2. Gherkin 시나리오 설계 및 문서화 역량
3. 외부 의존 격리 전략 수립
4. 재현 가능한 테스트 환경 설계(Environment Parity)
5. 단일 커맨드/CI 기반 자동화 파이프라인 구축

## 리뷰 포인트

### 1. 프로젝트 분석 및 인수조건 정의
- "1일차 테스트 작성 경험이 이번 분석에 어떤 도움이 되었나요?"
- "프로젝트의 핵심 도메인 흐름을 어떻게 파악했나요?"
- "외부 의존성(GiftDelivery 등)은 어떻게 식별했나요?"
- "왜 모든 기능이 아니라 이 기능을 선택했나요?"
- "리스크를 어떻게 판단했나요?"
- "실패 시나리오는 무엇을 보호하나요?"
- "이 시나리오가 깨지면 실제 서비스에 어떤 영향이 있나요?"

### 2. 시나리오 설계 (Gherkin)
- "AC가 시나리오에 자연스럽게 매핑되나요?"
- "Given/When/Then이 '행위/관찰 결과' 중심인가요, '구현' 중심인가요?"
- "1일차 테스트와 비교했을 때 Gherkin의 장점을 느끼셨나요?"
- "Background/Scenario Outline을 왜 사용(또는 미사용)했나요?"
- "중복 Step이 늘어나는 것을 어떻게 통제했나요?"

### 3. Step Definition 설계
- “Step이 너무 많은 일을 하지 않나요?” (하나의 Step이 여러 검증/여러 API 호출을 숨기지 않는지)
- “Step이 과도하게 기술적이지 않나요?” (도메인 용어 vs HTTP/DTO 중심)
- “Step 간 결합이 강하지 않나요?” (순서 의존, 공유 상태 과다)
- “테스트 실패 시 어디가 문제인지 빠르게 드러나나요?” (로그/리포트/어설션 위치)

### 4. 외부 의존 전략
- "GiftDelivery 같은 외부 의존을 어떻게 처리했나요?"
- "Mock? Stub? Fake? 어떤 테스트 더블을 선택했고 그 이유는?"
- "왜 실제 시스템을 사용하지 않았나요?"
- "테스트 더블이 실제 계약을 충분히 반영하고 있나요?"
- "계약 변경 시 어떻게 대응할 수 있나요?"
- "테스트 더블이 과도하게 단순화된 것은 아닌가요?"

### 5. 환경 패리티(Environment Parity)
- “로컬과 CI가 정말 동일하게 실행되나요?”
- “수동 단계가 숨어있지 않나요?”
- “CI에서 실패할 가능성은 없나요?” (포트 충돌, readiness, 환경변수 누락)
- “환경이 재현 불가능해질 위험은 없나요?”

### 6. 자동화 전략
- “한 줄 명령으로 실행 가능한가요?”
- “실패 시 디버깅은 어떻게 하나요?”
- “테스트가 flaky해질 가능성은 없나요?” (시간/랜덤/비동기)
- “병렬 실행 시 문제가 생기지 않나요?”

### 7. AI 활용
- “AI가 제안한 전략을 그대로 사용했나요?”
- “AI의 제안을 어떻게 검증했나요?”
- “AI가 틀린 부분은 무엇이었나요?”
